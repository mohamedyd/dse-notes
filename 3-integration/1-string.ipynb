{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Search\n",
    "\n",
    "We ended the previous lecture by noting the challenges of working with unstructured strings. The treatment of string attributes in a database requires further discussion. For string attributes, databases are good at looking up or joining on exact matches using hashing. For more complicated conditions (e.g., find all near matches in two tables), this story is a little more complicated. We start our study of string-searching algorithms, sometimes called string-matching algorithms. These are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger set of strings.\n",
    "\n",
    "A string similarity measure is a function that evalautes how similar (or different) two strings are. Let $s$ and $t$ be two strings from the same alphabet. A string similarity function takes the two strings as arguments and returns a continuous value 0 to 1 $m(s,t) \\mapsto [0,1]$. 1 implies the strings are equal and 0 implies they are maximally different.\n",
    "\n",
    "For the purposes of this lecture, we will only consider strings from the lower-case latin alphabet and similarity measures over them. \n",
    "\n",
    "## Tokenization and Preprocessing\n",
    "The first step in designing a good string similarity metric is the process of tokenization. This considers the minimum granularity of matching that we will consider. Think of these like the important features of a string that are worth considering. For example, we might tokenize a string on word boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'a', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "st = 'the quick brown fox jumped over a lazy dog'\n",
    "print(st.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we might tokenize a string on character boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', ' ', 'b', 'r', 'o', 'w', 'n', ' ', 'f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 'e', 'd', ' ', 'o', 'v', 'e', 'r', ' ', 'a', ' ', 'l', 'a', 'z', 'y', ' ', 'd', 'o', 'g']\n"
     ]
    }
   ],
   "source": [
    "st = 'the quick brown fox jumped over a lazy dog'\n",
    "print(list(st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general tokenization class takes a string as input and returns an iterator over tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "cow\n",
      "jumped\n",
      "over\n",
      "the\n",
      "moon\n"
     ]
    }
   ],
   "source": [
    "class WordTokenization():\n",
    "    '''\n",
    "    This class tokenizes a string\n",
    "    into a sequence of words.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input):\n",
    "        self.input = input\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.it = iter(self.input.split())\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return next(self.it)\n",
    "\n",
    "\n",
    "for i in WordTokenization('the cow jumped over the moon'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times, we will further process the output of the returned tokens. For example, we may not be interested in articles or \"stop words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cow\n",
      "jumped\n",
      "over\n",
      "moon\n"
     ]
    }
   ],
   "source": [
    "class StopWordFilter():\n",
    "    '''\n",
    "    This removes stop words from a tokenized\n",
    "    stream\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input):\n",
    "        self.input = input\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.it = iter(self.input)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        rtn = next(self.it)\n",
    "        if rtn == 'a' or rtn == 'an' or rtn == 'the':\n",
    "            return self.__next__()\n",
    "        else:\n",
    "            return rtn\n",
    "\n",
    "for i in StopWordFilter(WordTokenization('the cow jumped over the moon')):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could similarly filter the tokenized output to only return unique words (remove duplicates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "cow\n",
      "jumped\n",
      "over\n",
      "moon\n"
     ]
    }
   ],
   "source": [
    "class DedupFilter():\n",
    "    '''\n",
    "    This removes stop words from a tokenized\n",
    "    stream\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input):\n",
    "        self.input = input\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.it = iter(self.input)\n",
    "        self.prev = set()\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        rtn = next(self.it)\n",
    "        \n",
    "        if rtn in self.prev:\n",
    "            return self.__next__()\n",
    "        else:\n",
    "            self.prev.add(rtn)\n",
    "            return rtn\n",
    "\n",
    "for i in DedupFilter(WordTokenization('the cow jumped over the moon')):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity \n",
    "Over a sequence of tokens one of the simplest (but ubiquitous!) similarity measures is Jaccard Similarity. Jaccard similarity, also known as Intersection over Union, is defined as the size of the intersection (how many unique tokens appear in both) divided by the size of the union (how many unique tokens appear in either)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def intersection(s_tokens, t_tokens):\n",
    "    count = 0\n",
    "    for s in DedupFilter(s_tokens):\n",
    "        for t in DedupFilter(t_tokens):    \n",
    "            if s == t:\n",
    "                count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "def union(s_tokens, t_tokens):\n",
    "    count = 0\n",
    "    for s in DedupFilter(itertools.chain(s_tokens,t_tokens)):\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def jaccard(s_tokens, t_tokens):\n",
    "    return intersection(s_tokens, t_tokens)/union(s_tokens, t_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "s = WordTokenization('the quick brown')\n",
    "t = WordTokenization('quick silver')\n",
    "\n",
    "print(jaccard(s,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "s = WordTokenization('the quick brown')\n",
    "t = WordTokenization('the quick brown')\n",
    "\n",
    "print(jaccard(s,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "s = WordTokenization('the quick brown')\n",
    "t = WordTokenization('a big dog')\n",
    "\n",
    "print(jaccard(s,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "s = WordTokenization('the quick the')\n",
    "t = WordTokenization('the big dog')\n",
    "\n",
    "print(jaccard(s,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Jaccard Matcher\n",
    "The Jaccard similarity can be used in more complex operators. Given two lists of strings `strlist1` and `strlist2`, find all pairs of strings with a Jaccard similarity of at least `thresh`. Why might we want to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "strlist1 = ['the big bear', 'dog in the woods', 'alphabet soup', 'kermit surprise']\n",
    "strlist2 = ['big bear', 'dogs', 'alphabet soup 1', 'kermit surprise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is often a core primitive in data integration problems. Data integration is the combination of technical and business processes used to combine data from disparate sources into meaningful and valuable information. Sometimes the two sources might have slighly differently descriptions for the same real world entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaccardMatchOperator:\n",
    "\n",
    "\n",
    "    def __init__(self, input, thresh):\n",
    "        '''\n",
    "        Takes in a tuple of input iterators (i1,i2)\n",
    "        '''\n",
    "        self.in1, self.in2 = input\n",
    "        self.thresh = thresh\n",
    "        # a list of iterators\n",
    "        \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        Initializes the iterators and fetches the first element\n",
    "        '''\n",
    "\n",
    "        self.it1 = iter(self.in1) # initialize the first input\n",
    "        self.it2 = iter(self.in2) # initialize the second input\n",
    "        \n",
    "        self.i = next(self.it1)\n",
    "        self.j = next(self.it2)\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Below are two helper methods. Conceptually,\n",
    "    we are running the following patter:\n",
    "    for i in it1:\n",
    "        for j in it2:\n",
    "            if jaccard(i,j) > thresh:\n",
    "                return (i,j)\n",
    "    To implement this with iterators, we need two\n",
    "    helper methods _reset_or_inc2 (go back to the\n",
    "    beginning of the inner for loop), or _inc1_or_end\n",
    "    (increment the first for loop or stop)\n",
    "    \"\"\"\n",
    "\n",
    "    def _reset_or_inc2(self):\n",
    "        try:\n",
    "            self.j = next(self.it2)\n",
    "\n",
    "        except StopIteration:\n",
    "            self.it2 = iter(self.in2)\n",
    "            self.j = next(self.it2)\n",
    "            self._inc1_or_end()\n",
    "\n",
    "    def _inc1_or_end(self):\n",
    "        try:\n",
    "            self.i = next(self.it1)\n",
    "        except StopIteration:\n",
    "            self.i = None\n",
    "            self.j = None\n",
    "\n",
    "\n",
    "    def __next__(self):\n",
    "        '''\n",
    "        The next method fetches the next element\n",
    "        '''\n",
    "\n",
    "        rtn = (self.i, self.j)\n",
    "\n",
    "        self._reset_or_inc2()\n",
    "\n",
    "        # skip non-pairs\n",
    "        if rtn[0] == None:\n",
    "            raise StopIteration()\n",
    "\n",
    "        if jaccard(WordTokenization(rtn[0]),WordTokenization(rtn[1])) <= self.thresh:\n",
    "            return self.__next__()\n",
    "        else:\n",
    "            return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the big bear <-> big bear\n",
      "alphabet soup <-> alphabet soup 1\n",
      "kermit surprise <-> kermit surprise\n"
     ]
    }
   ],
   "source": [
    "for i,j in JaccardMatchOperator((strlist1, strlist2),0.5):\n",
    "    print(i,\"<->\",j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
