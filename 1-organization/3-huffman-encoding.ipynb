{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploiting Data Skew\n",
    "\n",
    "In the previous lecture, we showed how to exploit redundancy by building a lookup table that mapped one of 4 possible strings to a 2-bit key:\n",
    "\n",
    "| Color | key |\n",
    "|-------|-------|\n",
    "| Red   | 00 |\n",
    "| Green  | 01|\n",
    "| Blue  | 10|\n",
    "| Black  | 11|\n",
    "\n",
    "Suppose, we new that almost all of the colors in our list were Red. This scheme is a bit wasteful because we allocate the same number of bits to the most popular colors as the least. Is there a way to dynamically adjust our encoding to use less space to store strings that we know will show up more often.\n",
    "\n",
    "This is not a niche problem. Almost all real-world data is skewed for example the characters in the engligh language follow this distribution: \n",
    "![title](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/English_letter_frequency_%28alphabetic%29.svg/1024px-English_letter_frequency_%28alphabetic%29.svg.png)\n",
    "\n",
    "## A Naive Approach\n",
    "\n",
    "Let's try manually adjust our encoding to scale with popularity. Consider the lookup table before (with the strings annotated with popularity). We sort the strings from most to least popular, assign them a binary key, and then prune the leading zeros.\n",
    "\n",
    "| Color | key |\n",
    "|-------|-------|\n",
    "| Red (0.8)   | 0 |\n",
    "| Green (0.02)  |11|\n",
    "| Blue (0.15)  | 10|\n",
    "| Black (0.3)  | 1|\n",
    "\n",
    "Our previous dictionary encoding approach requires 2-bits per string on average. Let's calculate the average (or expected) storage cost for this new encoding:\n",
    "\n",
    "$$0.8*1 + 0.3*1 + 0.15 * 2 + 0.02 * 2 = 1.44$$\n",
    "\n",
    "This means that simply changing the encoding compresses the data by about 40%. But, what goes wrong with this approach? Suppose, we retrieved the bits 10--how do we decode that? Is it \"Black, Red\" or is it \"Blue\". This is the core issue with this naive approach that there is amiguity in the decoding.\n",
    "\n",
    "## Prefix-Free Codes\n",
    "A careful reader might note that we could side-step this problem (and still get a compression benefit) by encoding the data slightly differently:\n",
    "\n",
    "| Color | key |\n",
    "|-------|-------|\n",
    "| Red (0.8)   | 0 |\n",
    "| Green (0.02)  |101|\n",
    "| Blue (0.15)  | 11|\n",
    "| Black (0.3)  | 10|\n",
    "\n",
    "$$0.8*1 + 0.3*2 + 0.15 * 2 + 0.02 * 3 = 1.76$$\n",
    "\n",
    "If read the data left to right, there is no longer any ambiguity 0101111101 => \"0,Red\", \"10,Black\", \"11,Blue\", \"11,Blue\", \"101 Green\". We simply scan the list until we find a \"full\" key in our lookup table. This works because each of the keys is prefix-free. A prefix-free encoding is a key that requires that there is no whole code word in the system that is a prefix (initial segment) of any other code word in the system. Luckily, we don't have to design such schemes by hand. There are algorithms that can find very efficient prefix-free encodings. \n",
    "\n",
    "## Huffman Coding\n",
    "Huffman's encodes data by building a binary tree where the leaves are strings. The basic strategy is to go bottom up. Keep on grouping the lowest probability strings into common prefixes until there are none left:\n",
    "\n",
    "![title](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Huffman_coding_example.svg/1920px-Huffman_coding_example.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily express this algorithm in code. The first step is to write the bottom up algorithm that builds the tree of symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expect input of the form [(.95,['s1']), (.05,['s2'])]\n",
    "\n",
    "def build_tree(strings):\n",
    "    '''Given a set of strings and their associated\n",
    "       probabilities build a tree with nested lists\n",
    "    '''\n",
    "    \n",
    "    while len(strings) > 1:\n",
    "        \n",
    "        strings.sort() #sort by \n",
    "        \n",
    "        p1,s1 = strings[0] #get lowest\n",
    "        p2,s2 = strings[1] #get lowest\n",
    "        \n",
    "        del strings[0]\n",
    "        del strings[0]\n",
    "        \n",
    "        strings.append((p1+p2, ((p1,s1),(p2,s2)) ))\n",
    "    \n",
    "    return strings[0]\n",
    "\n",
    "tree = build_tree([(.95,['s1']), (.02,['s2']), (.03,['s3']) ])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we have a top down traversal that assigns codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s2'] 11\n",
      "['s3'] 10\n",
      "['s1'] 0\n"
     ]
    }
   ],
   "source": [
    "def assign_codes(tree, code=''):\n",
    "    \n",
    "    p,s = tree\n",
    "    \n",
    "    if len(s) == 1:\n",
    "       print(s, code)\n",
    "    else:\n",
    "        assign_codes(s[0],code+'1')\n",
    "        assign_codes(s[1],code+'0')\n",
    "\n",
    "assign_codes(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
