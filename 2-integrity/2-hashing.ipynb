{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing\n",
    "A hash function is any function that can be used to map data of arbitrary size onto data of a fixed size. While the potential domain of the data could be huge, its practical domain is often small (i.e., the realized values in the dataset). So if we are careful about how we hash the data there is often a unique encoding for each value.\n",
    "\n",
    "So what does a typical hash function look like? Here is a hash function that hashes strings to integer values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii\n",
    "import random\n",
    "\n",
    "# maximum integer value\n",
    "MAXINT = 2**32-1\n",
    "\n",
    "# We need the next largest prime number above MAXINT\n",
    "NEXTPRIME = 4294967311\n",
    "\n",
    "# Two numbers that parametrize the hash\n",
    "A = random.randint(0, MAXINT)\n",
    "B = random.randint(0, MAXINT)\n",
    "\n",
    "def hashcode(st):\n",
    "    val = binascii.crc32(bytes(str(st),'utf-8')) & 0xffffffff\n",
    "    return (A * val + B) % NEXTPRIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run this hash function on a list of strings, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803015294\n",
      "3329900216\n",
      "1598068164\n",
      "3663318587\n",
      "803015294\n",
      "2477368713\n",
      "3523361111\n"
     ]
    }
   ],
   "source": [
    "for s in ['the','quick','brown','fox','the','lazy', 'dog']:\n",
    "    print(hashcode(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of interesting things to note in the output. First, the word 'the' predictably gets the same hash code (integer value). We can also restrict this value to be integers in a certain range (e.g., 0-50) by applying another remainder operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "16\n",
      "14\n",
      "37\n",
      "44\n",
      "13\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for s in ['the','quick','brown','fox','the','lazy', 'dog']:\n",
    "    print(hashcode(s) % 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code space (the range) is too small, then you get \"collisions\", namely, two unequal values get the same hash code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "6\n",
      "4\n",
      "7\n",
      "4\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for s in ['the','quick','brown','fox','the','lazy', 'dog']:\n",
    "    print(hashcode(s) % 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Efficient Matcher\n",
    "We can revisit the match operator from the previous lectures with a significantly more efficient implementation that uses hashing. This is the basis of the Hash Join algorithm. The hash join is an example of a join algorithm and is used in database systems. Hash joins are typically more efficient for larger result sets than nested loops, but can only be used for equality joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchOperator:\n",
    "\n",
    "\n",
    "    def __init__(self, input, codespace=50):\n",
    "        '''\n",
    "        Takes in a tuple of input iterators (i1,i2)\n",
    "        '''\n",
    "        self.in1, self.in2 = input\n",
    "        self.codespace = codespace\n",
    "        # a list of iterators\n",
    "\n",
    "        \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        Initializes the iterators and fetches the first element\n",
    "        '''\n",
    "\n",
    "        self.it1 = iter(self.in1) # initialize the first input\n",
    "        self.it2 = iter(self.in2) # initialize the second input\n",
    "        \n",
    "        self.hashtable = [[] for i in range(self.codespace)]\n",
    "        \n",
    "        #build the hash table\n",
    "        #for i,v in enumerate(it1):\n",
    "            #hash v and append i to a list\n",
    "            \n",
    "        for i, v in enumerate(self.it1):\n",
    "            self.hashtable[hashcode(v) % self.codespace].append(v)\n",
    "            \n",
    "        \n",
    "        #keep track of the bucket number and next value\n",
    "        self.nextval = next(self.it2)\n",
    "        self.nextb = 0\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    \n",
    "    def __next__(self):\n",
    "        '''\n",
    "        The next method fetches the next element\n",
    "        '''\n",
    "        probe = self.hashtable[hashcode(self.nextval) % self.codespace]\n",
    "        \n",
    "        if len(probe) <= self.nextb:\n",
    "            self.nextval = next(self.it2)\n",
    "            self.nextb = 0\n",
    "            return self.__next__()\n",
    "        elif probe[self.nextb] == self.nextval:\n",
    "            rtn = self.nextval\n",
    "            self.nextb += 1            \n",
    "            return (rtn, rtn)\n",
    "        else:\n",
    "            self.nextb += 1 \n",
    "            return self.__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this code, the input and the output behavior are exactly the same as before but it is a more efficient implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "for i in MatchOperator(([1,2,4,5],[5,4,3,6])):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is clearly faster than the \"nested loop\" version, but the problem is that it requires one of the iterators to completely fit in memory. How do we get around this dilema?\n",
    "\n",
    "## External Hashing\n",
    "The key idea is to partition the data into chunks that fit into memory. This partitioning is also done with hashing. The structure of this algorithm is very similar to the external sorting we saw before. We recursively subdivide the data until it fits into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iosim import *\n",
    "\n",
    "#A function to hash partition the data\n",
    "def partition(infile, partitions):\n",
    "    hashtable = [[] for i in range(partitions)]\n",
    "    output_files = []\n",
    "    \n",
    "    for i, v in enumerate(Load(infile)):\n",
    "        hashtable[hashcode(v) % partitions].append(v)\n",
    "    \n",
    "    for code in range(0,partitions):\n",
    "        filename = infile + \".\" + str(code)\n",
    "        \n",
    "        if len(hashtable[code]) > 0:\n",
    "            Flush(hashtable[code],filename)\n",
    "            output_files.append((filename, Size(filename)))\n",
    "    \n",
    "    return output_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This breaks up the file into k smaller components. However, some of the files might be very large due to collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('input.txt.0', 39), ('input.txt.1', 5), ('input.txt.2', 35)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition('input.txt', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to recursively subdivide these partitions until they all meet a given size limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('input.txt', 82),\n",
       " ('input.txt.0', 39),\n",
       " ('input.txt.0.0', 33),\n",
       " ('input.txt.0.0.0', 34),\n",
       " ('input.txt.0.0.0.0', 8),\n",
       " ('input.txt.0.0.0.1', 0),\n",
       " ('input.txt.0.0.0.2', 0),\n",
       " ('input.txt.0.0.1', 0),\n",
       " ('input.txt.0.0.2', 0),\n",
       " ('input.txt.0.1', 7),\n",
       " ('input.txt.0.2', 0),\n",
       " ('input.txt.1', 5),\n",
       " ('input.txt.2', 35),\n",
       " ('input.txt.2.0', 0),\n",
       " ('input.txt.2.1', 0),\n",
       " ('input.txt.2.2', 5)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def passes(infile, limit, partitions):\n",
    "    \n",
    "    file, size = infile\n",
    "    rtn = []\n",
    "    if size > limit:\n",
    "        for partfile in partition(file, partitions):\n",
    "            rtn.extend(passes(partfile,limit, partitions))\n",
    "    else:\n",
    "         rtn = [infile]\n",
    "            \n",
    "    return rtn\n",
    "\n",
    "passes((\"input.txt\", Size(\"input.txt\")), 10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lecture, we considered a disk (or more generally \"external memory\") api, where the system could `load` and `flush` data. We will add a new primitive to this api called `seek` which efficiently returns a single element at a given position in the disk file. The `seek` operation will be crucial to implement indexing. Indexing is a way to optimize performance of a database by minimizing the number of disk accesses required when a query is processed. An index or database index is a data structure which is used to quickly locate and access the data in a database table.\n",
    "\n",
    "## Seek\n",
    "We can really think of any storage device as a large array which can be indexed. In our model, we model the disk as files divided by lines. The `Seek` operator takes in an iterator of \"indices\" (think line numbers in a file) and selectively returns the line at that number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from iosim import *\n",
    "\n",
    "for val in Seek([2,0,1], 'test.csv'):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Seek` takes a line number and returns a value. What if we wanted to do the reverse (take a value and return the line numbers at which it occurs)? This is the basic concept of indexing. Given a value determine if the value exists on a disk system and if it does return the whole line(s). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Indexing\n",
    "You will learn about more complicated types of indexes in a database systems class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
