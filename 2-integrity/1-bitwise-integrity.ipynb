{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitwise Integrity\n",
    "\n",
    "Before we get into more complex definitions of data integrity, let's consider a simple thought experiment. Suppose, we went around the classroom and asked every student to write down an integer on a strip of paper. We put all of these strips of paper into a bag and give them to one student to keep safe for a week. The next week, we want to confirm that all of the strips of paper are still there (and have their original values!). What if the student lost one of the tickets and made up a value to ensure the count would be the same? Short of writing down every number and putting it in a safe place, how would we efficiently test if some modification happened to the strips of paper (either a lost strip or a manipulated strip)?\n",
    "\n",
    "## Naive Technique\n",
    "Let's consider the following algorithm. When students add their strips of paper to the bag, we incrementally calculate the total value of all of the numbers. When all of the students have added their strips of paper to the bag, we will add the final sum (called a Checksum) on a special colored strip of paper to the bag. When we retrieve the bag next week we simply sum up all of the values again and compare them to the checksum. If the checksum itself is missing then the question is moot as the bag is already untrustworthy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test original True\n",
      "Test manipulated False\n"
     ]
    }
   ],
   "source": [
    "def checksum(data):\n",
    "    return (data, sum(data))\n",
    "\n",
    "def verify(data):\n",
    "    return (sum(data[0]) == data[1])  \n",
    "\n",
    "v = checksum([1,3,4,5,6])\n",
    "print('Test original', verify(v))\n",
    "\n",
    "#manipulated\n",
    "v[0][3] = 7\n",
    "print('Test manipulated',verify(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, this scheme takes a large amount of data and summarizes it into a signature, which we use for data verification. The verification is substantially more storage efficient than redundantly recording every piece of information. Why won't this particular signature work? Most obviously we can easily trick this system by strategically modifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test strategic True\n"
     ]
    }
   ],
   "source": [
    "v = checksum([1,3,4,5,6])\n",
    "\n",
    "#manipulated\n",
    "v[0][3] = 7\n",
    "v[0][4] = 4\n",
    "print('Test strategic',verify(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is called a \"collision\", when two different data sets have the same checksum. In a sense, collisions are guaranteed to happen because the checksum is much smaller than the original data. Let's ignore this issue for a bit and think of an equally important but more subtle problem with the above scheme.\n",
    "\n",
    "There is a glaring systems problem with this approach. A sum of two integers is not guaranteed to be an integer (it's data may exceed the number of bits needed to represent an integer). \n",
    "\n",
    "## Cyclic Checksum\n",
    "We can define a new checksum function that restricts the range of the checksum to a fixed number of bits by using the modulo operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 3, 4, 5, 6], 1)\n",
      "([1, 3, 4, 5, 6], 5)\n"
     ]
    }
   ],
   "source": [
    "def checksum(data, length):\n",
    "    #length is the size in bits needed to store the checksum\n",
    "    return (data, (sum(data) % (2**length - 1) ))\n",
    "\n",
    "v = checksum([1,3,4,5,6], 2)\n",
    "print(v)\n",
    "\n",
    "v = checksum([1,3,4,5,6], 3)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obvious consequence of this is that you are even more susceptible missing errors. Random errors that simply have the same modulus can result in the same checksum. The more bits that you have the safer you are, but it is still unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Better Approach\n",
    "\n",
    "The key issue is that summing up a bunch of numbers washes out the variation in those numbers, i.e., 2 + 4 is the same as 3 + 3. The natural thing to do instead is to concatenate the numbers instead of of summing them up. Let's assume that every number had the same number of digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(data):\n",
    "    strs = [str(d) for d in data] #turn it into strings\n",
    "    return int(''.join(strs))\n",
    "\n",
    "def checksum(data, length):\n",
    "    #length is the size in bits needed to store the checksum\n",
    "    return (data, (concat(data) % (2**length - 1) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the concatenation it is harder to determine how a small change affects the checksum. Decrementing one of the numbers leads to a drastic change in the checksum: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 3, 4, 5, 6], 1)\n",
      "([1, 2, 4, 5, 6], 6)\n"
     ]
    }
   ],
   "source": [
    "v = checksum([1,3,4,5,6], 4)\n",
    "print(v)\n",
    "\n",
    "v = checksum([1,2,4,5,6], 4)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular approach is called a cylic redundancy check and can actually be implemented in fully boolean arithmetic (instead of digits you work with bits) (summations and divisions are constructed with the XOR and AND operators). \n",
    "\n",
    "https://en.wikipedia.org/wiki/Cyclic_redundancy_check\n",
    "\n",
    "While substantially better than simply summing up the numbers it is still not robust to strategic manipulation. It is possible to use mathematics to determine which numbers to manipulate and get the same final checksum. In the next lecture, we'll start to dig deeper in to this problem.\n",
    "\n",
    "\n",
    "## Hashing\n",
    "The CRC is a particularly efficient calculation and can be executed in hardware. Sometimes, we may want stronger checks that are a bit less efficient. The study of designing such functions is the part of the general area of \"hashing\" in computer science. \n",
    "\n",
    "Like a checksum, a hash function is any function that can be used to map data of arbitrary size onto data of a fixed size. The key to designing a good hash function is that it must be disorderly---small changes in the input map to very large and unpredictable changes in the output. Thus, while collisions certainly will exist they will be evenly spread out over the entire domain of the data. Why does this work? While the potential domain of the data could be huge, its practical domain is often small (i.e., the realized values in the dataset). So if we are careful about how we hash the data there is often a unique encoding for each value. \n",
    "\n",
    "Simple hash functions often take a form similar to the CRC checks that we saw above. They sum and multiply the bits in a data point and take the modulus. Here is a hash function that hashes strings to integer values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223513802\n",
      "2366007334\n",
      "1223513802\n"
     ]
    }
   ],
   "source": [
    "import binascii\n",
    "import random\n",
    "\n",
    "# maximum integer value\n",
    "MAXINT = 2**32-1\n",
    "\n",
    "# We need the next largest prime number above MAXINT\n",
    "NEXTPRIME = 4294967311\n",
    "\n",
    "# Two numbers that parametrize the hash\n",
    "A = random.randint(0, MAXINT)\n",
    "B = random.randint(0, MAXINT)\n",
    "\n",
    "def hashcode(st):\n",
    "    val = binascii.crc32(bytes(str(st),'utf-8')) & 0xffffffff\n",
    "    return (A * val + B) % NEXTPRIME\n",
    "\n",
    "print(hashcode('the bear'))\n",
    "print(hashcode('a bear'))\n",
    "print(hashcode('the bear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, a couple of differences with the CRC scheme above. First, the modulus is a prime number. Intuitively, prime numbers make sense as a modulus because there are less ways to accidentally find numbers that cleanly divide with them. We might have to further restrict these hash functions to a smaller domain (for example to fit into a fixed size list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, ['the bear'], None, ['a bear'], None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "lst = [None]* N\n",
    "lst[hashcode('the bear') % N] = ['the bear']\n",
    "lst[hashcode('a bear') % N] = ['a bear']\n",
    "\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data structure is called a hashmap and illustrates the power of hashing. To determine whether we have already inserted a string into our data structure before, we can simply test to see if there is an entry corresponding to its hash code and only compare to those elements that have the same code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the weights A and B are randomly generated. In fact, different A's and B's provide an entire family of hash functions. If the A's and B's are picked independently, we can almost model them as independently random variables: the probability of collision from each one is independent. Thus, even if we have \"weak\" hash functions, as long as we can generate a independent functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
